{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadeelmu/deeplearning/blob/main/Semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WLMNhNzrvZN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_51fkJmjea86"
      },
      "source": [
        "<style>\n",
        "r { color: Red }\n",
        "o { color: Orange }\n",
        "g { color: Green }\n",
        "b { color: Blue }\n",
        "l { color: lighblue }\n",
        "</style>\n",
        "\n",
        "\n",
        "<html>\n",
        "<body>\n",
        "<table style=\"border: 0; rules=none; font-size:28px\">\n",
        "<tr>\n",
        "<th rowspan=5><img width=\"200px\", height=\"70px\" src=\"https://raw.githubusercontent.com/camma-public/multibypass140/master/static/camma_logo_tr.png\"/></th>\n",
        "<td colspan=2 style=\"font-size:16px; color:blue; font-weight:bold\"><h1><b>Deep Learning for Computer Vision</b></h1></td>\n",
        "<th rowspan=5><img width=\"200px\", height=\"130px\" src=\"https://community.sap.com/legacyfs/online/storage/blog_attachments/2019/10/283545_NeuralNetwork_R_blue.png\"/></th>\n",
        "</tr>\n",
        "<tr><td>Instructor:</td><td>Dr. Chinedu Nwoye</td></tr>\n",
        "<tr><td colspan=2>(c) Research Group CAMMA</td></tr>\n",
        "<tr><td colspan=2>University of Strasbourg</td></tr>\n",
        "<tr><td>Website:</td><td><g>http://camma.u-strasbg.fr</g></td></tr>\n",
        "<tr><td colspan=4 style=\"text-align:centre; background-color:black; font-weight:bold\"><center><h3><o>Semantic Segmentation</o></td></center></tr>\n",
        "</table>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\n",
        "--------\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- In this lab session we will train a deep learning model for semantic segmentation.\n",
        "- You will practise and learn how to organize your custom dataset and build data loading pipeline for your experiment.\n",
        "- You will learn to design metrics and loss functions for your tasks.\n",
        "- You will practise and learn to train and monitor your model optimization on your dataset.\n",
        "- Your will be required to complete all the TODO tasks in this exercise. Ask the instructor if you get in a hole.\n",
        "- You will be required to innovate tricks using all you have learnt in previous classes to improve your model performance.\n",
        "\n",
        "### GPU activation\n",
        "\n",
        "- Be sure to have cuda enabled from your computer.\n",
        "\n",
        "### Imports\n",
        "\n",
        "- Every experiments starts with importing the required libraries.\n",
        "- Check and see what libraries you don't know their usage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atijLt93ea87",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Resize, CenterCrop, ConvertImageDtype, Normalize\n",
        "from torchmetrics import JaccardIndex\n",
        "import matplotlib.pyplot as plt\n",
        "import importlib as ipl\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import urllib\n",
        "from timeit import default_timer as timer\n",
        "import gc\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image, ImageColor\n",
        "\n",
        "# check the PyTorch version;\n",
        "print(\"PyTorch version: \", torch.__version__)\n",
        "print(\"torchvision version: \", torchvision.__version__)\n",
        "\n",
        "# check the GPU support; shold be yes\n",
        "print(\"Is GPU available?: \", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mvrH8JMOp2mN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd4efe0-0dab-46bc-d942-57205bb13f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Dataset\n",
        "In this lab session, we will use two datasets:\n",
        "\n",
        "<br><hr><hr><br>\n",
        "\n",
        "1.   Semantic Segmentation of Underwater Imagery (SUIM): A Large Scale Dataset for underwater Semantic Segmentation\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-datasets-images/1557385/2565717/11a9865b8b2d8f692e45f51c3468a339/dataset-cover.jpg?t=2021-08-28-11-25-08\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
        "\n",
        "<br><hr><hr><br>\n",
        "\n",
        "2.   m2caiSeg: A Surgical Dataset for Semantic Segmentation of Laparoscopic Images\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-datasets-images/1025913/1728780/461f50a0dde9c8d5fdc4b6f8459165e4/dataset-cover.png?t=2020-12-09-22-58-52\" alt=\"Drawing\" style=\"width: 14%\"/>\n",
        "\n",
        "<br><hr><hr><br>\n",
        "\n",
        "**[1.1] Download data**\n",
        "-\n",
        "Make a choice of dataset to use and download it"
      ],
      "metadata": {
        "id": "7BBvFWNpTZV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO9eNFVBpdtS"
      },
      "outputs": [],
      "source": [
        "# run to download dataset and unzip the folder\n",
        "\n",
        "options = [\"suim\", \"m2caiseg\"]\n",
        "\n",
        "dataset_choice = ... # TODO\n",
        "\n",
        "if dataset_choice == 'suim':\n",
        "  !wget --backups 0 https://seafile.unistra.fr/f/17341900616245fb8c0c/?dl=1 --content-disposition &&  unzip -qq suim.zip -d /suim\n",
        "\n",
        "elif dataset_choice ==  'm2caiseg':\n",
        "  !wget --backups 0 https://seafile.unistra.fr/f/a09df056d97f4de7a507/?dl=1 --content-disposition &&  unzip -qq m2caiSeg.zip -d /\n",
        "\n",
        "else:\n",
        "    os.error(\"No Dataset selected from {}\".format(options))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/suim\")"
      ],
      "metadata": {
        "id": "PkpusUuiqADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1.2] Dataset Meta***\n",
        "- Information about the dataset.\n",
        "- This usually provided on the dataset documentation, website or publication releasing the dataset."
      ],
      "metadata": {
        "id": "UTR_o6l_RQyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUIM META\n",
        "suim_class = [\"background_waterbody\", \"human_divers\", \"aquatic_plants_and_sea_grass\", \"wrecks_and_ruins\", \"robots\", \"reefs_and_invertebrates\", \"fish_and_vertebrates\", \"sea_floor_and_rocks\"]\n",
        "suim_color = [(0,0,0), (0,0,255), (0,255,0), (0,255,255), (255,0,0), (255,0,255), (255,255,0), (255,255,255)]\n",
        "\n",
        "# M2CAI META\n",
        "m2caiseg_class = [\"Unknown\",\"Grasper\", \"Bipolar\", \"Hook\", \"Scissors\", \"Clipper\", \"Irrigator\", \"Specimen bag\", \"Trocars\", \"Clip\", \"Liver\", \"Gallbladder\", \"Fat\",\"Upper Wall\", \"Intestine\",\"Artery\",\"Bile\", \"Blood\", \"Black\"]\n",
        "m2caiseg_color = [[170,0,85],[0,85,170],[0,85,255],[0,170,85],[0,255,85],[0,255,170],[85,0,170],[85,0,255],[170,85,85],[170,170,170],[85,170,0],[85,170,255],[85,255,0],[85,255,170],[170,0,255],[255,0,255],[255,255,0],[255,0,0],[0,0,0]]\n",
        "\n",
        "# DATA ROOT\n",
        "filepaths = {\n",
        "    'suim': '/suim',\n",
        "    'm2caiseg': '/m2caiSeg',\n",
        "}\n",
        "\n",
        "# COMBINE\n",
        "color_maps = {\n",
        "    \"suim\": suim_color,\n",
        "     'm2caiseg': m2caiseg_color,\n",
        "\n",
        "}\n",
        "\n",
        "class_maps = {\n",
        "    \"suim\": suim_class,\n",
        "     'm2caiseg': m2caiseg_class,\n",
        "}\n",
        "\n",
        "image_dirs = {\n",
        "    \"suim\": \"images\",\n",
        "    'm2caiseg': 'images'\n",
        "}\n",
        "\n",
        "mask_dirs = {\n",
        "    \"suim\": \"masks\",\n",
        "    'm2caiseg': 'groundtruth',\n",
        "}\n",
        "\n",
        "image_exts = {\n",
        "    \"suim\": \".jpg\",\n",
        "     'm2caiseg': '.jpg'\n",
        "}\n",
        "\n",
        "\n",
        "mask_exts = {\n",
        "    \"suim\": \".bmp\",\n",
        "     'm2caiseg': '.png'\n",
        "}\n",
        "\n",
        "filepath = filepaths[dataset_choice]\n",
        "color_mapping = color_maps[dataset_choice]\n",
        "classes = class_maps[dataset_choice]\n",
        "image_dir = image_dirs[dataset_choice]\n",
        "mask_dir = mask_dirs[dataset_choice]\n",
        "image_ext = image_exts[dataset_choice]\n",
        "mask_ext = mask_exts[dataset_choice]\n"
      ],
      "metadata": {
        "id": "39_r4XHbOup4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1.3] Helper functions**\n",
        "- Some functions you will be needing"
      ],
      "metadata": {
        "id": "pnYMMpCuSvAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ******** IMPORTANT HELPER FUNCTIONS FOR THIS LAB ****************\n",
        "\n",
        "# Show the contents of your dataset folder\n",
        "# It is important to see how your dataset is organized.\n",
        "# You can also do this using file explorer\n",
        "def show_folder_structure(startpath):\n",
        "    assert os.path.exists(startpath), \"File path does not exist!\"\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for j,f in enumerate(sorted(files[:3])):\n",
        "            print('{}{}'.format(subindent, f))\n",
        "            if j==2:\n",
        "              print(subindent,'...')\n",
        "              print('{}{}'.format(subindent, files[-1]))\n",
        "\n",
        "\n",
        "# Display images and labels\n",
        "def display_image(images, titles):\n",
        "    f, axes = plt.subplots(1, len(images), sharey=True)\n",
        "    for i in range(len(images)):\n",
        "        axes[i].imshow(images[i])\n",
        "        axes[i].set_title(titles[i], fontsize=8, color= 'blue')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Convert segmentation mask from RGB to Semantic channel.\n",
        "# RGB channel = 3 (reg, green, blue)\n",
        "# Semantic channel = N, where N = number of classes, one channel per class\n",
        "def rgb_to_semantic(image, color_mapping):\n",
        "    image_array = np.array(image)\n",
        "    repeated_image = np.repeat(image_array[:, :, np.newaxis, :], len(color_mapping), axis=2) # [rgb channels] x number of classes\n",
        "    repeated_mapping = np.repeat(np.array(list(color_mapping))[np.newaxis, np.newaxis, :, :], image_array.shape[0], axis=0) # [semantic channels] x number of classes\n",
        "    maskND = np.all(repeated_image == repeated_mapping, axis=-1).astype(np.uint8) # Equality broadcast\n",
        "    return maskND\n",
        "\n",
        "\n",
        "# Convert segmentation mask with semantic channel to a single channel\n",
        "# Use NumPy broadcasting to assign the keys to the matching pixels\n",
        "# Each pixel takes the class categorical value\n",
        "def nD_to_1D(maskND):\n",
        "    mask1D = np.argmax(maskND, axis=-1)\n",
        "    return mask1D\n",
        "\n",
        "\n",
        "# Convert semantic channel mask to rgb channel image\n",
        "# Create an array of RGB values corresponding to keys in the mapping\n",
        "# And Map the keys in the image to their corresponding RGB values\n",
        "def semantic_to_rgb(mapped_image, color_mapping):\n",
        "    color_array = np.array(color_mapping, dtype=np.uint8)\n",
        "    rgb_image = color_array[mapped_image]\n",
        "    return rgb_image\n",
        "\n"
      ],
      "metadata": {
        "id": "WmeX8Br4nfSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1.4] Data Inspection**\n",
        "\n",
        "- The first step in training a model is understanding your data.\n",
        "- We will analyze the data to see what it look like and how we can use it."
      ],
      "metadata": {
        "id": "DkmzONlkVXzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# folder structure exploration\n",
        "\n",
        "show_folder_structure(filepath)\n"
      ],
      "metadata": {
        "id": "QndHsSpvVplV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment this if you want to use explorer to explore your dataset\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lq4gRBuxMaE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check data size\n",
        "\n",
        "train_size = len(os.listdir(os.path.join(filepath, \"train\", image_dir)))\n",
        "val_size = len(os.listdir(os.path.join(filepath, \"val\", image_dir)))\n",
        "test_size = len(os.listdir(os.path.join(filepath, \"test\", image_dir)))\n",
        "\n",
        "print(\"Size | train: {}, val: {}, test:{}\".format(train_size, val_size, test_size))"
      ],
      "metadata": {
        "id": "pNR695uDfenN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show one image and label to see what they are like, in actual sense, you need to check many examples.\n",
        "\n",
        "selected_img_file = ... # TODO\n",
        "selected_msk_file = ... # TODO\n",
        "\n",
        "selected_img_file = os.listdir(os.path.join(filepath, \"train\", image_dir))[2]\n",
        "selected_msk_file = os.listdir(os.path.join(filepath, \"train\", mask_dir))[2]\n",
        "\n",
        "img1_url = os.path.join(filepath, \"train\", image_dir, selected_img_file)\n",
        "msk1_url = os.path.join(filepath, \"train\", mask_dir, selected_msk_file)\n",
        "rgb_img1 = Image.open(img1_url).convert(\"RGB\")\n",
        "rgb_msk1 = Image.open(msk1_url).convert(\"RGB\")\n",
        "rgb_img1 = np.array(rgb_img1)\n",
        "rgb_msk1 = np.array(rgb_msk1)\n",
        "\n",
        "\n",
        "display_image(images=[rgb_img1, rgb_msk1], titles=['image', 'mask'])\n"
      ],
      "metadata": {
        "id": "WCX3sYNvfo60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your data shape and distribution\n",
        "# This is very important to understand your data\n",
        "\n",
        "print(\"Image shape = \", rgb_img1.shape)\n",
        "print(\"Mask shape = \", rgb_msk1.shape)\n",
        "\n",
        "print(\"Image distribution: [Min = {}, Mean = {}, Max = {}] \".format(rgb_img1.min(), rgb_img1.mean(), rgb_img1.max()))\n",
        "print(\"Mask distribution: [Min = {}, Mean = {}, Max = {}] \".format(rgb_msk1.min(), rgb_msk1.mean(), rgb_msk1.max()))\n"
      ],
      "metadata": {
        "id": "WcQya0gcT_BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Label Processing\n",
        "# You are to process your label to have it in a format that your model can use.\n",
        "# You have seen the shape and it has RGB channel but your mode will need a semantic channel\n",
        "# Semantic label means N channel where N = number of classes\n",
        "\n",
        "# 1. Convert RGB channel to semantic channel mask.\n",
        "# To do this, you must know the color code mapping of semantic class.\n",
        "semantic_mask_ND = rgb_to_semantic(rgb_msk1, color_mapping)\n",
        "\n",
        "# 2. Convert semantic mask to single channel mask\n",
        "# We can now visualize the converted semantic channel mask,\n",
        "# So we convert it to single channel with each pixel having the channel index with maximum value\n",
        "semantic_mask_1D = nD_to_1D(semantic_mask_ND)\n",
        "\n",
        "# 3. Recover rgb mask\n",
        "# We can convert the single channel easily to the RGB channel to visual the mask.\n",
        "# If you didn't get back your original RGB mask, it means your label processing code is not correct.\n",
        "recovered_rgb_msk1 = semantic_to_rgb(semantic_mask_1D, color_mapping)\n",
        "\n",
        "# 4. Visualize\n",
        "display_image(images=[rgb_img1, rgb_msk1, semantic_mask_1D, recovered_rgb_msk1],\n",
        "              titles=['Image', 'RGB mask', \"Semantic mask\", \"Reversed RGB mask\"])\n",
        "\n",
        "\n",
        "# NB: We will only need the semantic channel mask for model training, the rest is for visualization purpose."
      ],
      "metadata": {
        "id": "8M6Le-d8uoeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1.3] Data loader**\n",
        "- Now, you can comfortably manipulate your dataset, you can build a dataloader to handle that.\n",
        "- Previously, we relied on PyTorch's `torchvision.datasets.ImageFolder` library to load our data.\n",
        "- But this data set is not structured as previous where every image is in its respective label folder.\n",
        "- So, we have to write a custom data loader that can load our dataset\n",
        "- The goal is to return a pair (image, mask) from your dataset at every call.\n",
        "\n",
        "**[1.2] Data preparation**\n",
        "- We will be packaging the data into a PyTorch datasets.\n",
        "- We will add basic preprocessing of image resizing,\n",
        "- We will split the data into train/val/test sets,\n",
        "- We will build a data loader for each split with a batch size of 32 for speed and convenience,\n",
        "- We will also create a small size train data."
      ],
      "metadata": {
        "id": "uirvAKSH9RC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a dataset class that delivers images and correponding ground truth segmentation masks\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    # Your Dataset class will inherit torch.utils.data.Dataset\n",
        "    # There are 3 most important function to overider here\n",
        "    # 1. `init` function: This prepare your dataset like a stack of data that are indexable\n",
        "    # 2. `len` function: This return the total number of data you have\n",
        "    # 3. `getitem` function: This return individual (image, target) on each call\n",
        "    # You can write other functions that can help these 3 fulfill their duties\n",
        "    def __init__(self, root_dir=\"/muis\", data_split=\"train\", image_dir=\"images\", mask_dir=\"masks\",\n",
        "                 image_ext=\".jpg\", mask_ext=\".bmp\", image_transforms=ToTensor(), mask_transforms=ToTensor(),\n",
        "                 color_mapping=None):\n",
        "        np.random.seed(13)\n",
        "        image_paths = os.path.join(root_dir, data_split, image_dir, \"*{}\".format(image_ext))\n",
        "        self.images = sorted(glob.glob(image_paths))\n",
        "        self.masks  = [img.replace(image_dir, mask_dir).replace(image_ext, mask_ext) for img in self.images]\n",
        "        self.image_transforms = image_transforms\n",
        "        self.mask_transforms = mask_transforms\n",
        "        self.color_mapping = color_mapping\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "        msk = Image.open(self.masks[index]).convert(\"RGB\")\n",
        "        img = self.image_transforms(img)\n",
        "        msk = self.mask_transforms(msk)\n",
        "        msk = self.rgb_to_semantic_mask(msk)\n",
        "        return img, msk\n",
        "\n",
        "\n",
        "    def rgb_to_semantic_mask(self, mask):\n",
        "        mask  = (mask * 255.0).long()\n",
        "        mask_flat = mask.view(3, -1).t()\n",
        "        mapper = torch.tensor(list(self.color_mapping))\n",
        "        indices = torch.argmax((mask_flat.unsqueeze(1) == mapper.unsqueeze(0)).all(dim=-1).int(), dim=-1)\n",
        "        mask1D = indices.view(mask.shape[1], mask.shape[2])\n",
        "        maskND = torch.eye(len(self.color_mapping), dtype=torch.float32)[mask1D].permute(2,0,1)\n",
        "        return maskND\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ1UnBsdlYgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Transformation\n",
        "# This is where you can write all your data preprocessing and data augmentation function\n",
        "# It is always preferable to have different transformation for the training and evaluation sets\n",
        "\n",
        "mean_imagenet = [0.485, 0.456, 0.406]\n",
        "std_imagenet  = [0.485, 0.456, 0.406]\n",
        "base_size = 200\n",
        "img_size = [224, 224]\n",
        "\n",
        "\n",
        "train_image_transforms = torchvision.transforms.Compose([\n",
        "    ToTensor(),\n",
        "    # CenterCrop(base_size),\n",
        "    Resize(size=(224,224)),\n",
        "    Normalize(mean=mean_imagenet, std=std_imagenet),\n",
        "])\n",
        "\n",
        "train_mask_transforms = torchvision.transforms.Compose([\n",
        "    ToTensor(),\n",
        "    # CenterCrop(base_size),\n",
        "    Resize(size=(224,224), interpolation=torchvision.transforms.InterpolationMode.NEAREST_EXACT),\n",
        "])\n",
        "\n",
        "eval_image_transforms = torchvision.transforms.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(size=(224,224)),\n",
        "    Normalize(mean=mean_imagenet, std=std_imagenet),\n",
        "])\n",
        "\n",
        "eval_mask_transforms = torchvision.transforms.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(size=(224,224), interpolation=torchvision.transforms.InterpolationMode.NEAREST_EXACT),\n",
        "])\n"
      ],
      "metadata": {
        "id": "qL7cvUknOUIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your dataloader # Be sure your data loader works as desired before using it to train your model\n",
        "\n",
        "BATCH_SIZE = ... # TODO. Give a batch size to your data loader\n",
        "\n",
        "\n",
        "# build dataset for different data split\n",
        "train_dataset = MyDataset(root_dir=filepath, data_split=\"train\", image_dir=image_dir, mask_dir=mask_dir,\n",
        "                          image_ext=image_ext, mask_ext=mask_ext, image_transforms=train_image_transforms,\n",
        "                          mask_transforms=train_mask_transforms, color_mapping=color_mapping)\n",
        "\n",
        "val_dataset = MyDataset(root_dir=filepath, data_split=\"val\", image_dir=image_dir, mask_dir=mask_dir,\n",
        "                          image_ext=image_ext, mask_ext=mask_ext, image_transforms=eval_image_transforms,\n",
        "                          mask_transforms=eval_mask_transforms, color_mapping=color_mapping)\n",
        "\n",
        "test_dataset = MyDataset(root_dir=filepath, data_split=\"test\", image_dir=image_dir, mask_dir=mask_dir,\n",
        "                          image_ext=image_ext, mask_ext=mask_ext, image_transforms=eval_image_transforms,\n",
        "                          mask_transforms=eval_mask_transforms, color_mapping=color_mapping)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Build their loader, include a batch size, data shuffling and any other feature.\n",
        "train_dataloader = ... # TODO\n",
        "val_dataloader = ... # TODO\n",
        "test_dataloader = ... # TODO\n",
        "\n",
        "\n",
        "# Check one sample\n",
        "image_i, mask_i = next(iter(train_dataloader))\n",
        "\n",
        "\n",
        "print(\"Image shape = {} | Mask shape = {}\".format(image_i.shape, mask_i.shape))\n",
        "\n",
        "# Torch uses the channel-first tensor, we can transpose to channel-last to visualize\n",
        "image_1 = image_i[0].permute(1,2,0)\n",
        "mask_1 = mask_i[0].permute(1,2,0)\n",
        "\n",
        "\n",
        "# Convert semantic mask to singel channel and final to rgb channel to visualize\n",
        "semantic_mask_1D = nD_to_1D(mask_1)\n",
        "recovered_rgb_msk = semantic_to_rgb(semantic_mask_1D, color_mapping)\n",
        "\n",
        "# Plot\n",
        "display_image(images=[image_1, recovered_rgb_msk],\n",
        "               titles=['Image', 'Target mask'])\n"
      ],
      "metadata": {
        "id": "G611H3La-DX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Model\n",
        "- We will use a state-of-the-art segmentation model known as DeepLabv3\n",
        "- We will leverage transfer learning from MS COCO dataset\n",
        "- We will do surgery on the model to adapt to our dataset number of class"
      ],
      "metadata": {
        "id": "gIA11Mcacupy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get state of the arts model from torchvision repository\n",
        "\n",
        "seg_model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True, progress=True, num_classes=21, aux_loss=None)\n",
        "\n",
        "\n",
        "# Let's see the architecture of the model\n",
        "# we look at only the classifier module\n",
        "print(seg_model.classifier)"
      ],
      "metadata": {
        "id": "uqqfb7k0cR9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the last layer to have output channel matching your dataset classes\n",
        "classes = list(range(19))\n",
        "myClassifier = ... # TODO: Build a convolution layer to replace the layer convolution in your model. Keep in mind the number of input and output filters\n",
        "\n",
        "seg_model.classifier[4] = myClassifier\n",
        "\n",
        "# Inspect the architecture to see your new model\n",
        "print(seg_model.classifier)"
      ],
      "metadata": {
        "id": "of6ICJn4cPr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let check if all the model parameter are trainable\n",
        "\n",
        "def is_trainable(module):\n",
        "    status = np.all([param.requires_grad for param in module.parameters()])\n",
        "    return status\n",
        "\n",
        "\n",
        "print(\"Model Trainable = \", is_trainable(seg_model))"
      ],
      "metadata": {
        "id": "bkAlhFS2Z7YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the model so we don't train it because we are doing transfer learning with partial finetunning\n",
        "for param in seg_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# We want to finetune all the classifier layers only, so unfreeze that part\n",
        "for param in ... : # TODO: Choose wisely layers to train\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "print(\"Model's Backbone Trainable = \", is_trainable(seg_model.backbone))\n",
        "print(\"Model's Classifier Trainable = \", is_trainable(seg_model.classifier))"
      ],
      "metadata": {
        "id": "TRN5VpXWbgen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run an inference to see how much it can do without training\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Check one sample\n",
        "image_i, gt_mask_i = next(iter(train_dataloader))\n",
        "input_image = ... # TODO: put your data to a device\n",
        "\n",
        "# inference\n",
        "seg_model = seg_model.to(device)\n",
        "seg_model.... # TODO: Switch your model into an evaluation mode\n",
        "pd_mask = ... # TODO: Make an inference on the input image\n",
        "pd_mask_i = pd_mask['out'].cpu()\n",
        "print(\"Image shape = {} | GT Mask shape = {} | Pred Mask shape = {}\".format(image_i.shape, gt_mask_i.shape, pd_mask_i.shape))\n",
        "\n",
        "# Torch uses the channel-first tensor, we can transpose to channel-last to visualize\n",
        "image_1   = image_i[0].... # TODO\n",
        "\n",
        "# Convert semantic mask to singel channel and final to rgb channel to visualize\n",
        "gt_mask_1  = gt_mask_i.softmax(1).argmax(1)[0]\n",
        "pd_mask_1  = pd_mask_i.softmax(1).argmax(1)[0]\n",
        "gt_rgb_msk = semantic_to_rgb(gt_mask_1, color_mapping)\n",
        "pd_rgb_msk = semantic_to_rgb(pd_mask_1, color_mapping)\n",
        "\n",
        "# Plot\n",
        "display_image(images=[image_1, gt_rgb_msk, pd_rgb_msk],\n",
        "               titles=['Image', 'Target mask', \"Predicted mask\"])"
      ],
      "metadata": {
        "id": "bg27qnZUf2nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Training and Evaluation\n",
        "\n",
        "- You have seen that pretrained model understanding some low-level details like shape, edges, etc. but they lack the class semantics.\n",
        "- We will train this model by finetuning it on the new dataset for few epochs/iteration to adapt it to the daraser domain."
      ],
      "metadata": {
        "id": "3fzHpwURi-vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For ease of mastery, we will reuse our previous training and evaluation code\n",
        "\n",
        "\n",
        "# Training step\n",
        "def train_step(inputs, labels, model, criterion, optimizer, device):\n",
        "    model.train() # set to training mode\n",
        "    inputs = inputs.to(device) # device allocation\n",
        "    labels = labels.to(device) # device allocation\n",
        "    outputs = model(inputs)['out'] # inference\n",
        "    # TODO: In 4 lines of code, do the following:\n",
        "    # 1. zero the parameter gradients in your optimizer\n",
        "    # 2. compute loss\n",
        "    # 3. backpropagation\n",
        "    # 4. optimization step\n",
        "    batch_loss = loss.item() * inputs.size(0) # loss performance\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "# Evaluation step\n",
        "def eval_step(inputs, labels, model, metrics, device):\n",
        "    model.eval() # set to evaluation mode\n",
        "    with torch.no_grad(): # stop gradient computation\n",
        "      inputs = inputs.to(device) # device allocation\n",
        "      labels = labels.to(device) # device allocation\n",
        "      outputs = model(inputs)['out'] # inference\n",
        "      preds = outputs.softmax(1).argmax(1)\n",
        "      targets = labels.softmax(1).argmax(1)\n",
        "      batch_iou = metrics(preds, targets) # iou performance\n",
        "      batch_accs = torch.sum(preds == targets).float()/targets.numel() # accuracy performance\n",
        "    return batch_accs.item() * inputs.size(0), batch_iou.item() * inputs.size(0)\n",
        "\n",
        "\n",
        "# train and validate cycle\n",
        "def train_model(model, criterion, optimizer, scheduler, metrics, device, num_epochs=25):\n",
        "    start = time.time()\n",
        "    epoch_loss = []\n",
        "    epoch_accs = []\n",
        "    epoch_ious = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train loop\n",
        "        running_loss = 0.0\n",
        "        running_accs = 0\n",
        "        for inputs, labels in train_dataloader: # Iterate over data.\n",
        "            loss = train_step(inputs, labels, model, criterion, optimizer, device)\n",
        "            running_loss += loss\n",
        "        scheduler.step() # decay learning rate\n",
        "        train_epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_loss.append(train_epoch_loss)\n",
        "\n",
        "        # validation loop\n",
        "        running_accs = 0\n",
        "        running_ious = 0\n",
        "        for inputs, labels in val_dataloader: # Iterate over data.\n",
        "            acc, iou = eval_step(inputs, labels, model, metrics, device)\n",
        "            running_accs += acc\n",
        "            running_ious += iou\n",
        "        val_epoch_acc = running_accs / len(val_dataset)\n",
        "        val_epoch_iou = running_ious / len(val_dataset)\n",
        "        epoch_accs.append(val_epoch_acc)\n",
        "        epoch_ious.append(val_epoch_iou)\n",
        "\n",
        "        print('Epoch {}/{} >> TRAIN Loss: {:.4f} | VAL IoU: {:.4f} Acc: {:.4f}'.format(\n",
        "                epoch, num_epochs-1, train_epoch_loss, val_epoch_iou, val_epoch_acc))\n",
        "        print('-' * 10)\n",
        "\n",
        "    # Reports\n",
        "    time_elapsed = time.time() - start\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    fig = plt.figure()\n",
        "\n",
        "    ax = fig.add_subplot(131)\n",
        "    intervals = np.arange(len(epoch_loss))\n",
        "    ax.plot(intervals, epoch_loss)\n",
        "    ax.set_xlabel(\"epochs\")\n",
        "    ax.set_ylabel(\"segmentation loss\")\n",
        "\n",
        "    ax = fig.add_subplot(132)\n",
        "    intervals = np.arange(len(epoch_accs))\n",
        "    ax.plot(intervals, epoch_accs)\n",
        "    ax.set_xlabel(\"epochs\")\n",
        "    ax.set_ylabel(\"validation accuracy\")\n",
        "\n",
        "    ax = fig.add_subplot(133)\n",
        "    intervals = np.arange(len(epoch_ious))\n",
        "    ax.plot(intervals, epoch_ious)\n",
        "    ax.set_xlabel(\"epochs\")\n",
        "    ax.set_ylabel(\"validation IoU\")\n",
        "\n",
        "    print(\"Evaluation Accuracy = {} and IoU = {}\".format(epoch_accs[-1], epoch_ious[-1]))\n",
        "\n",
        "    # clear GPU cache memory\n",
        "    torch.cuda.empty_cache()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def visualize_preds(model, dataloader, choice, device):\n",
        "    iterloader = iter(dataloader)\n",
        "    if choice >= len(iterloader):\n",
        "        choice = 0\n",
        "    for _ in range(choice-1):\n",
        "        next(iterloader)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs, labels = next(iterloader)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)['out'] # inference\n",
        "        preds   = outputs.softmax(1).argmax(1)\n",
        "        targets = labels.softmax(1).argmax(1)\n",
        "    for img, pred, target in zip(inputs, preds, targets):\n",
        "        img = img.permute(1,2,0).cpu()\n",
        "        target = target.cpu()\n",
        "        pred = pred.cpu()\n",
        "        print(np.unique(pred))\n",
        "        gt_rgb_msk = semantic_to_rgb(target, color_mapping)\n",
        "        pd_rgb_msk = semantic_to_rgb(pred, color_mapping)\n",
        "        display_image(images=[img.cpu(), gt_rgb_msk, pd_rgb_msk],\n",
        "                  titles=['Image', 'Target mask', \"Predicted mask\"])\n",
        "    return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gMrewB6VdsZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3.2] Train and validate**"
      ],
      "metadata": {
        "id": "zVvx9fHLtBGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate\n",
        "LEARNING_RATE = 0.07\n",
        "\n",
        "# resource allocation\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "seg_model  = seg_model.to(device)\n",
        "\n",
        "# loss function\n",
        "criterion = ... # TODO: Define your loss function\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "# metrics function\n",
        "metrics = JaccardIndex(task='multiclass', num_classes=len(classes))\n",
        "metrics = metrics.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer =  ... # TODO: Define an optimizer\n",
        "\n",
        "# learning rate decay scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# training\n",
        "trained_model = train_model(seg_model, criterion, optimizer, scheduler, metrics, device, num_epochs=25)"
      ],
      "metadata": {
        "id": "uftZ6qAPrZ9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on test set"
      ],
      "metadata": {
        "id": "KX2xjw62vNZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on the test data\n",
        "running_ious = 0\n",
        "running_accs = 0\n",
        "for inputs, labels in ... : # TODO: Iterate over test data.\n",
        "    acc, iou = eval_step(inputs, labels, trained_model, metrics, device)\n",
        "    running_ious += iou\n",
        "    running_accs += acc\n",
        "train_epoch_acc = running_accs / len(test_dataset)\n",
        "train_epoch_iou = running_ious / len(test_dataset)\n",
        "\n",
        "print('TEST Acc: {:.4f} | IoU {:.4f}'.format( train_epoch_acc, train_epoch_iou ))\n"
      ],
      "metadata": {
        "id": "K7ET9P2ANC5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the images and prediction, manually find the images that your model failed on.\n",
        "iterloader = iter(test_dataloader)\n",
        "N = len(test_dataloader)\n",
        "choice = random.choice(list(range(N)))\n",
        "print(\"Chosing batch {} out of {} batches\".format(choice, N))\n",
        "\n",
        "visualize_preds(trained_model, test_dataloader, choice, device)"
      ],
      "metadata": {
        "id": "Qe1IyrNVNgYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWW87X67e0CU"
      },
      "source": [
        "# Section 4: Performance improvement\n",
        "\n",
        "**[4.1] Model babysitting**\n",
        "\n",
        "**Question**\n",
        "- How can we improve the performance of the model?\n",
        "- *This case can be similar to your internship projects and real-world problems.*\n",
        "\n",
        "\n",
        "\n",
        "**Solutions**\n",
        "- There can be a lot of things to try, I will list them and you will try them unassisted to see which works best for you.\n",
        "- If you can train your model on small dataset and get a test performance equal or higher than the one we trained on the large training data, we will get a beautiful gift from me.\n",
        "\n",
        "\n",
        "**List of strategies to try:**\n",
        "1. Data augumentation\n",
        "2. Weight regularization\n",
        "3. Hyperparameter tuning (e.g. learning rate, batch size, weight decay, etc. *find more*)\n",
        "4. Early stopping\n",
        "5. Dropout\n",
        "6. Batch normalization\n",
        "7. Intuitive objective loss function\n",
        "8. Some network layer modification\n",
        "9. Change of model or parts of the model\n",
        "10. Full retrain\n",
        "11. *Find more*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}