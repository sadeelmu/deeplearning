{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadeelmu/deeplearning/blob/main/Rnn_applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEtTCE-1CUUU"
      },
      "source": [
        "# TP 5b - RNN applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdVMmKo9CoCW"
      },
      "source": [
        "This lab session will show you how RNNs can be applied to text data. A lot of the workload is dedicated to preparing data; this is true for deep learning in general, but text requires special considerations. Do not worry too much about training the actual RNN, as that is the generic and easy part you have already done in the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lICrualNYJR"
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "import pickle\n",
        "\n",
        "random.seed(2111994)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck2zlidHDI1m"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9U73Vp9DMRG"
      },
      "source": [
        "The dataset we will be using is a corpus of user-submitted movie reviews from **IMDB**.\n",
        "\n",
        "The task you will attempt to perform is **sentiment analysis**. A 0 or 1 label is provided for each review, indicating whether the user rated the movie as bad (0) or good (1) overall.\n",
        "\n",
        "Run the cell below to load the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4hzeGvnAksN"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "N_LIMIT_TRAIN = 10000\n",
        "N_LIMIT_TEST = 5000\n",
        "\n",
        "ds_train = list(tfds.load('imdb_reviews', split='train').take(N_LIMIT_TRAIN))\n",
        "ds_test = list(tfds.load('imdb_reviews', split='test').take(N_LIMIT_TEST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY1xUFa_F2mv"
      },
      "source": [
        "Preview one item from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2CR9mpvGJUe"
      },
      "source": [
        "ds_test[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yImWM-uDEnY_"
      },
      "source": [
        "A few caveats about the format:\n",
        "- those are lists of dictionaries with two keys each, *label* and *text*\n",
        "- the corresponding values are TF tensors; they need to be converted before usage\n",
        "- the *text* is encoded (as signaled by the *b* before the quote), meaning an extra conversion step is necessary\n",
        "\n",
        "The next cell will take care of those conversions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLFTUch5BRvS"
      },
      "source": [
        "def convert_dict(d):\n",
        "    d[\"label\"] = d[\"label\"].numpy()\n",
        "    d[\"text\"] = d[\"text\"].numpy().decode(\"utf-8\")\n",
        "    return d\n",
        "\n",
        "ds_train = [convert_dict(d) for d in ds_train]\n",
        "ds_test = [convert_dict(d) for d in ds_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPo1Vu2nGZn_"
      },
      "source": [
        "## 1 - Preprocessing\n",
        "\n",
        "The purpose of this section is to turn your text data into something usable for an RNN: **vectors**. The steps are the following:\n",
        "- build a **tokenizer** for breaking text into words\n",
        "- determine a reduced **vocabulary** for your text data\n",
        "- convert your text to lists of integers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEzh5E-GH4Gr"
      },
      "source": [
        "\n",
        "The first step is to *cleanly* break down your text into words. This is actually more difficult than it seems; you will try your code on the following string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tR2BSv4I_cX"
      },
      "source": [
        "sample_string = ds_test[0][\"text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaOv2_iMI5pO"
      },
      "source": [
        "We will rely on a library called *NLTK*. Import it with the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixlFdv1_XelO"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v5qwc4KJkcj"
      },
      "source": [
        "#### QUESTION 1\n",
        "Use *nltk.word_tokenizer* to separate the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aorwy9vEkcBP"
      },
      "source": [
        "sample_string_tokens = # TODO\n",
        "print(sample_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrzUzi9J0FG"
      },
      "source": [
        "#### QUESTION 2\n",
        "\n",
        "Eliminate punctuation from each word and convert it to lowercase. Eliminate the annoying *br* special character as well, if it is there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Clx0MZpkcm3"
      },
      "source": [
        "sample_string_tokens_clean = # TODO\n",
        "print(sample_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRxdyx57KVkF"
      },
      "source": [
        "#### QUESTION 3\n",
        "\n",
        "Complete *tokenize* so that it returns a list of separate cleaned-up (lowercase, no punctuation, no *br*) words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30wni4eqkdW6"
      },
      "source": [
        "def tokenize(string_in):\n",
        "    # TODO\n",
        "    return string_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SejTBXLbHUu9"
      },
      "source": [
        "## Vocabulary\n",
        "\n",
        "Now that you are able to break down your text into words, you will determine what **vocabulary** you will be using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIc8vgg9Ku4w"
      },
      "source": [
        "#### QUESTION 4\n",
        "\n",
        "Randomly sample 2000 reviews from your entire dataset using *random.sample*. How many words do they contain (*tokenize* will help)? How many **distinct** words do they contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd1_wK9kkf_E"
      },
      "source": [
        "full_dataset = ds_train + ds_test\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39w0zkKFLoJ8"
      },
      "source": [
        "We don't need to use so many words; we will focus on a high enough number to cover 95% of the text in those 2000 reviews you sampled, and discard the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5fSvrIXMC_r"
      },
      "source": [
        "#### QUESTION 5\n",
        "\n",
        "\n",
        "Rank the words you encountered by number of appearances, and display the ranking on a plot (maybe just the top 100 for better readability)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRZ0vHo_kgyQ"
      },
      "source": [
        "appearances = {}\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Ue-rhnMZeD"
      },
      "source": [
        "It looks like most of the text in the reviews uses only very few words. The word distribution you just saw obeys something called [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law). Weirdly enough this phenomenon happens in any language, not just English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JoHJELLMlqI"
      },
      "source": [
        "#### QUESTION 6\n",
        "\n",
        "Append enough words to *vocab_list* to cover 95% of the text in *sampled_reviews*. Complete the code for *vocab_dict* so that it matches each word to its index in *vocab_list*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtmVcwPlHjR"
      },
      "source": [
        "vocab_list = []\n",
        "vocab_list[0] = \"___\"\n",
        "# TODO: fill the rest\n",
        "vocab_dict = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x76g0mrBOCoB"
      },
      "source": [
        "#### QUESTION 7\n",
        "\n",
        "Time to put it together; convert the string in every item in *train* and *test* to a list of indices (use *vocab_dict*) - discard words that do not belong to your vocabulary. **Pad with zeros or crop to give all lists a length of 320**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOXM-LV3lIVh"
      },
      "source": [
        "ds_train = # TODO\n",
        "ds_test = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXq1iSJ9RKS4"
      },
      "source": [
        "## 2 - Vectorization\n",
        "\n",
        "We've successfully converted the text from the dataset to integer sequences. The next step is to turn those into sequences of \"meaningful\" **word vectors**, ready to throw into an RNN. This wouldn't be easy to get from scratch; to speed things up we'll rely on a set of pretrained word vectors. Load them with the following cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JCP2ooUSMSf"
      },
      "source": [
        "!wget --content-disposition https://seafile.unistra.fr/f/1daee01e85904416878e/?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeNLS8Wktnzy"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"word_vectors.pickle\", \"rb\") as f:\n",
        "    word_vecs = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1rl0oIhSPQb"
      },
      "source": [
        "This dictionary maps words to 250-D vectors. They were generated by an embedding trained using the Word2Vec methodology on a text corpus from Wikipedia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYMHjaDaSovm"
      },
      "source": [
        "#### QUESTION 8\n",
        "\n",
        "Play with your word vectors for a bit. Look up the vectors for \"queen\", \"cool\", \"warm\". Which word in *word_vecs* is the closest (use the *dist* function provided below) to *cold + (new - old)*? Why does that make sense? You may try other combinations if you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3TYcJ0ROYp_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuM2hA6YVb2M"
      },
      "source": [
        "#### QUESTION 9\n",
        "\n",
        "Build a (n_vocab, 250) numpy array; the i-th row should contain the vector for the i-th word in your vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywZuNwFxOdrb"
      },
      "source": [
        "embedding_array = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrLRGuXxVmWv"
      },
      "source": [
        "We can convert this to a tf.Tensor. We will use this later during training and testing to convert indices to vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reYslm0qiQfi"
      },
      "source": [
        "embedding_tensor = tf.Variable(embedding_array, trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWBjB785ez13"
      },
      "source": [
        "## 3 - Training\n",
        "\n",
        "The RNN model below is virtually the same as in the previous lab, except for two things: *tf.nn.embedding_lookup* converts the integer indices coming from the input to the corresponding word vectors, and a fully connected layer is inserted before the actual RNN to make the vector even smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkCXDXFgPwVM"
      },
      "source": [
        "class RNN_model(tf.keras.Model):\n",
        "  def __init__(self, memory_size):\n",
        "    super().__init__()\n",
        "    self._dense_1 = tf.keras.layers.Dense(\n",
        "      units=8,\n",
        "      activation=\"relu\"\n",
        "    )\n",
        "    self._cell = tf.keras.layers.LSTMCell(memory_size)\n",
        "    self._rnn = tf.keras.layers.RNN(self._cell)\n",
        "    self._dense_2 = tf.keras.layers.Dense(\n",
        "      units=N_CLASSES,\n",
        "      activation=\"softmax\"\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    res = x\n",
        "    res = tf.nn.embedding_lookup(embedding_tensor, res)\n",
        "    res = self._dense_1(res)\n",
        "    res = self._rnn(res)\n",
        "    res = self._dense_2(res)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jfetq9FiRHq"
      },
      "source": [
        "Let's wrap the data into tf Datasets as usual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lcUC5LkaJB"
      },
      "source": [
        "x_train = [d[\"text\"] for d in ds_train]\n",
        "y_train = [d[\"label\"] for d in ds_train]\n",
        "x_test = [d[\"text\"] for d in ds_train]\n",
        "y_test = [d[\"label\"] for d in ds_train]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJTsnrlkjABM"
      },
      "source": [
        "#### QUESTION 10\n",
        "Train the RNN for 10 epochs and evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjrHyRMLkEZE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}